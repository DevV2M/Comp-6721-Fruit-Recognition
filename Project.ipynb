{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9595516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "import graphviz\n",
    "#I Have problems with packages try running without this command \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import local_binary_pattern\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pathlib\n",
    "from torchvision.transforms import ToTensor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn import semi_supervised  \n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "3365b4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "#checking for device\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "235358a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_features(image, target_size):\n",
    "    # Convert the image to the HSV color space\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the number of bins for each channel in the histogram\n",
    "    hue_bins = 8\n",
    "    saturation_bins = 8\n",
    "    value_bins = 8\n",
    "\n",
    "    # Calculate the color histogram for each channel\n",
    "    hue_hist = cv2.calcHist([hsv_image], [0], None, [hue_bins], [0, 180])\n",
    "    saturation_hist = cv2.calcHist([hsv_image], [1], None, [saturation_bins], [0, 256])\n",
    "    value_hist = cv2.calcHist([hsv_image], [2], None, [value_bins], [0, 256])\n",
    "\n",
    "    # Normalize the histograms\n",
    "    cv2.normalize(hue_hist, hue_hist, 0, 1, cv2.NORM_MINMAX)\n",
    "    cv2.normalize(saturation_hist, saturation_hist, 0, 1, cv2.NORM_MINMAX)\n",
    "    cv2.normalize(value_hist, value_hist, 0, 1, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    color_features = np.concatenate((hue_hist.flatten(), saturation_hist.flatten(), value_hist.flatten()))\n",
    "\n",
    "    # Resize the color features to the target size\n",
    "    if len(color_features) < target_size:\n",
    "        color_features = np.pad(color_features, (0, target_size - len(color_features)), mode='constant')\n",
    "    elif len(color_features) > target_size:\n",
    "        color_features = color_features[:target_size]\n",
    "\n",
    "    #print(\"Color features:\", color_features.shape)\n",
    "\n",
    "    return color_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c57fa583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_shape_features(image, target_size):\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply binary thresholding to obtain a binary image\n",
    "    _, binary_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours in the binary image\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Initialize a list to store shape features\n",
    "    shape_features = []\n",
    "\n",
    "    # Iterate over the contours\n",
    "    for contour in contours:\n",
    "        # Calculate contour-based features\n",
    "        area = cv2.contourArea(contour)\n",
    "        perimeter = cv2.arcLength(contour, True)\n",
    "        _, _, width, height = cv2.boundingRect(contour)\n",
    "        aspect_ratio = width / float(height) if height != 0 else 0\n",
    "        circularity = 4 * np.pi * area / (perimeter ** 2) if perimeter != 0 else 0\n",
    "\n",
    "        # Append the shape features to the list\n",
    "        shape_features.extend([area, perimeter, aspect_ratio, circularity])\n",
    "\n",
    "    # Convert the shape features list to a numpy array\n",
    "    shape_features = np.array(shape_features)\n",
    "\n",
    "    # Resize the shape features to the target size\n",
    "    if len(shape_features) < target_size:\n",
    "        shape_features = np.pad(shape_features, (0, target_size - len(shape_features)), mode='constant')\n",
    "    elif len(shape_features) > target_size:\n",
    "        shape_features = shape_features[:target_size]\n",
    "\n",
    "    #print(\"Shape features:\", shape_features.shape)\n",
    "    return shape_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "3a0fbea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texture_features(image, target_size):\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate the Local Binary Pattern (LBP) for the grayscale image\n",
    "    radius = 1\n",
    "    n_points = 8 * radius\n",
    "    lbp_image = local_binary_pattern(gray_image, n_points, radius, method='uniform')\n",
    "\n",
    "    # Calculate the histogram of the LBP image\n",
    "    hist, _ = np.histogram(lbp_image.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "\n",
    "    # Normalize the histogram\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-7)\n",
    "\n",
    "    # Flatten and return the histogram as the texture feature vector\n",
    "    texture_features = hist.flatten()\n",
    "\n",
    "    # Resize the texture features to the target size\n",
    "    if len(texture_features) < target_size:\n",
    "        texture_features = np.pad(texture_features, (0, target_size - len(texture_features)), mode='constant')\n",
    "    elif len(texture_features) > target_size:\n",
    "        texture_features = texture_features[:target_size]\n",
    "\n",
    "    #print(\"Texture features:\", texture_features.shape)\n",
    "\n",
    "    return texture_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a131d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(image):\n",
    "    # Load and preprocess the image\n",
    "    # Assuming image is already loaded or you can use OpenCV to load it\n",
    "    preprocessed_image = image\n",
    "    #print(preprocessed_image)\n",
    "\n",
    "    # Extract features using different methods\n",
    "    color_features = extract_color_features(preprocessed_image,60)\n",
    "    shape_features = extract_shape_features(preprocessed_image,50)\n",
    "    texture_features = extract_texture_features(preprocessed_image,10)\n",
    "\n",
    "    # Combine the features into a single vector\n",
    "    #print(color_features.shape, shape_features.shape, texture_features.shape)\n",
    "    combined_features = np.concatenate((color_features, shape_features, texture_features),axis=None)\n",
    "\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "3f9f52ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#folder_path = \"/Users/hadi/Desktop/Concordia/Comp 6721/AIproject/fruits/training/Kiwi_Training\"\n",
    "\n",
    "def generate_features(image):\n",
    "    #image = cv2.imread(image_path)\n",
    "    #new_size = (32, 32)\n",
    "    #image = cv2.resize(image, new_size)\n",
    "    combined_features = combine_features(image)\n",
    "    return combined_features\n",
    "\n",
    "def check_label(element):\n",
    "    if element == 'Banana_Training' or element == \"Banana_Test\" or element == \"Banana_Validation\":\n",
    "        return 1\n",
    "    elif element == 'Kiwi_Training' or element == \"Kiwi_Test\" or element == \"Kiwi_Validation\":\n",
    "        return 2\n",
    "    elif element == 'Mango_Training' or element == \"Mango_Test\" or element == \"Mango_Validation\":\n",
    "        return 3\n",
    "    elif element == 'Orange_Training' or element == \"Orange_Test\" or element == \"Orange_Validation\":\n",
    "        return 4\n",
    "    elif element == 'Plum_Training' or element == \"Plum_Test\" or element == \"Plum_Validation\":\n",
    "        return 5\n",
    "    elif element == 'Apple_Training' or element == \"Apple_Test\" or  element == \"Apple_Validation\":\n",
    "        return 6\n",
    "    else:\n",
    "        return 0  # Return 0 if the element is not found in the list\n",
    "\n",
    "def loadImages(folder_path,class_):\n",
    "    #print(folder_path)\n",
    "    folder_path = folder_path\n",
    "    file_list = os.listdir(folder_path)\n",
    "    class_features = np.empty((0,121))\n",
    "    for file_name in file_list:\n",
    "         if file_name.endswith(\".jpg\") or file_name.endswith(\".png\"):\n",
    "             image_path = os.path.join(folder_path, file_name)\n",
    "             # Perform your image processing tasks here\n",
    "             image = cv2.imread(image_path)\n",
    "             new_size = (32, 32)\n",
    "             image = cv2.resize(image, new_size)\n",
    "             combined_features = combine_features(image)\n",
    "             #print(check_label(class_))\n",
    "             combined_features = np.append(combined_features, check_label(class_))\n",
    "             #print(combined_features.shape)\n",
    "             combined_features = np.expand_dims(combined_features, axis=0)\n",
    "             class_features = np.append(class_features, combined_features,axis=0)\n",
    "    #print(class_features[0])\n",
    "    return class_features\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "#Print the shape of the combined feature vector\n",
    "#loadImages(folder_path,\"Kiwi_Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b8b98cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/Users/hadi/Desktop/Concordia/Comp 6721/AIproject/fruits/training\"\n",
    "test_path = \"/Users/hadi/Desktop/Concordia/Comp 6721/AIproject/fruits/testing\"\n",
    "val_path = \"/Users/hadi/Desktop/Concordia/Comp 6721/AIproject/fruits/validation\"\n",
    "\n",
    "root_training=pathlib.Path(train_path)\n",
    "root_testing=pathlib.Path(test_path)\n",
    "root_val = pathlib.Path(val_path)\n",
    "\n",
    "\n",
    "def generate_feature_vector(root,path):\n",
    "    classes = []\n",
    "    features = np.empty((0,121))\n",
    "    labels = []\n",
    "    for class_dir in root.iterdir():\n",
    "        class_ = class_dir.name.split('/')[-1]\n",
    "        print(class_)\n",
    "        path_ = path +\"/\"\n",
    "        if(class_!=\".DS_Store\"):\n",
    "            print(class_)\n",
    "            path_ = path+\"/\"+class_\n",
    "            temp = loadImages(path_,class_)\n",
    "            path_ = \"\"\n",
    "            classes.append(class_)\n",
    "            features = np.append(features, temp,axis=0)\n",
    "                \n",
    "    return features \n",
    "#classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "#print(classes) #['Banana_Training', 'Kiwi_Training', 'Mango_Training', 'Orange_Training', 'Plum_Training', 'Apple_Training']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "316fc19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banana_Training\n",
      "Banana_Training\n",
      ".DS_Store\n",
      "Kiwi_Training\n",
      "Kiwi_Training\n",
      "Mango_Training\n",
      "Mango_Training\n",
      "Orange_Training\n",
      "Orange_Training\n",
      "Plum_Training\n",
      "Plum_Training\n",
      "Apple_Training\n",
      "Apple_Training\n",
      ".DS_Store\n",
      "Kiwi_Test\n",
      "Kiwi_Test\n",
      "Plum_Test\n",
      "Plum_Test\n",
      "Orange_Test\n",
      "Orange_Test\n",
      "Apple_Test\n",
      "Apple_Test\n",
      "Mango_Test\n",
      "Mango_Test\n",
      "Banana_Test\n",
      "Banana_Test\n",
      ".DS_Store\n",
      "Mango_Validation\n",
      "Mango_Validation\n",
      "Plum_Validation\n",
      "Plum_Validation\n",
      "Banana_Validation\n",
      "Banana_Validation\n",
      "Apple_Validation\n",
      "Apple_Validation\n",
      "Kiwi_Validation\n",
      "Kiwi_Validation\n",
      "Orange_Validation\n",
      "Orange_Validation\n",
      "(14676, 121)\n",
      "(4583, 121)\n",
      "(3677, 121)\n"
     ]
    }
   ],
   "source": [
    "#calculate size of training and testing images \n",
    "\n",
    "f = generate_feature_vector(root_training,train_path) #total training features \n",
    "t = generate_feature_vector(root_testing,test_path) #total testing features\n",
    "v = generate_feature_vector(root_val,val_path) #total val features\n",
    "\n",
    "print(f.shape)\n",
    "print(t.shape)\n",
    "print(v.shape)\n",
    "#train_count = len(glob.glob(train_path+\"/**/*.png\"))\n",
    "#test_count = len(glob.glob(test_path+\"/**/*.png\"))\n",
    "\n",
    "#print(train_count,test_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "b3a8b62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inlabled points, [ True  True  True ...  True  True  True]\n",
      "Size of labels: [-1. -1. -1. ... -1. -1. -1.]\n",
      "Count of unlabled data: 16433\n",
      "Actual labled data:  [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]\n",
      "Unlabled data:  [-1. -1.  3. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1.  3. -1. -1. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1.  3. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  3. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  3. -1.  3. -1. -1. -1. -1.  3.\n",
      " -1.  3. -1. -1. -1. -1. -1. -1. -1. -1.  3. -1. -1. -1. -1. -1.  3.  3.\n",
      " -1. -1.  3. -1.  3.  3. -1. -1. -1. -1. -1. -1. -1.  3. -1. -1. -1. -1.\n",
      " -1.  3. -1. -1. -1. -1. -1. -1. -1. -1. -1.  3. -1.  3. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1.  3. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  3. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1. -1. -1.  3.\n",
      " -1. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  3. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  3. -1.  3. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1.  3. -1. -1. -1.  3. -1. -1.  3. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1.  3. -1.  3. -1. -1. -1. -1. -1. -1. -1.  3. -1. -1.\n",
      " -1. -1.  3. -1. -1. -1. -1. -1. -1.  3. -1. -1. -1.  3. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.  3. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1. -1. -1.  3. -1.\n",
      " -1. -1. -1. -1.  3. -1. -1.  3. -1. -1. -1.  3. -1. -1. -1. -1. -1. -1.\n",
      "  3. -1. -1. -1. -1. -1. -1. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1.  3. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  3. -1. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  3. -1.\n",
      " -1. -1. -1. -1.  3. -1. -1. -1. -1.  3.  3. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  3. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1.  3.  3. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1.  3. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.  3.  3. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  3. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1.  3. -1.  3. -1. -1. -1. -1. -1.  3. -1. -1.\n",
      " -1. -1.  3. -1. -1.  3. -1. -1. -1. -1. -1. -1.  3. -1.  3. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1.  3. -1.  3.  3. -1. -1. -1. -1. -1. -1.  3. -1. -1.\n",
      " -1. -1. -1. -1. -1.  3.  3. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1.  3. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  3.\n",
      " -1.  3. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  3. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.  3. -1. -1. -1. -1. -1. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  3.  3. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.  3. -1. -1. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  3. -1. -1. -1. -1. -1. -1. -1. -1.  3.  3. -1. -1. -1. -1. -1. -1. -1.\n",
      "  3. -1.  3. -1. -1. -1. -1.  3. -1. -1. -1.  3. -1.  3. -1. -1. -1.  3.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  3. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  3. -1.\n",
      " -1. -1. -1. -1. -1. -1.  3. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1.  3. -1. -1. -1. -1. -1. -1. -1. -1.  3.  3. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  3. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "Semi Supervised Labels:  [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 6. 1. 3. 3. 5. 3. 3. 2. 3. 3. 3. 3.\n",
      " 3. 1. 6. 3. 3. 3. 3. 1. 1. 3. 1. 3. 3. 3. 3. 3. 3. 3. 1. 6. 3. 3. 3. 3.\n",
      " 3. 1. 3. 3. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 6. 3. 3. 3. 3. 1. 3. 3. 6.\n",
      " 3. 4. 3. 3. 3. 1. 3. 3. 3. 3. 3. 2. 3. 3. 3. 3. 1. 6. 1. 3. 3. 3. 3. 1.\n",
      " 1. 3. 3. 3. 3. 3. 1. 3. 3. 3. 6. 6. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 6.\n",
      " 3. 3. 2. 1. 3. 3. 3. 3. 3. 3. 3. 6. 6. 1. 3. 1. 3. 2. 3. 1. 3. 6. 3. 3.\n",
      " 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 2. 3. 3. 3. 6. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 1. 6. 3. 1. 3. 3. 3. 3. 6. 3. 3. 6. 6. 3. 2. 6. 1. 3. 3. 3. 3.\n",
      " 3. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 1. 1. 1. 1. 3. 3. 3. 6. 3. 3. 3. 3.\n",
      " 6. 3. 1. 6. 1. 6. 1. 3. 2. 3. 3. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 4. 3.\n",
      " 3. 3. 4. 3. 1. 3. 3. 3. 3. 3. 3. 6. 1. 3. 3. 1. 3. 6. 1. 3. 3. 3. 3. 1.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 6. 3. 3. 3. 6. 1. 3. 1. 3. 3. 3.\n",
      " 3. 6. 3. 3. 3. 3. 3. 3. 3. 1. 3. 3. 2. 3. 3. 3. 3. 3. 3. 3. 3. 6. 1. 3.\n",
      " 3. 1. 3. 3. 3. 3. 3. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 6. 3. 3. 2.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 2. 3. 3. 3. 1. 3. 3. 3. 3. 3. 6. 3. 3.\n",
      " 3. 3. 3. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 4. 1. 6. 3. 3. 1. 3. 3. 3. 3. 3.\n",
      " 3. 3. 6. 1. 1. 6. 3. 3. 3. 3. 3. 3. 6. 3. 3. 3. 3. 1. 3. 2. 3. 3. 2. 6.\n",
      " 3. 3. 3. 3. 3. 1. 1. 3. 2. 3. 3. 3. 3. 3. 1. 3. 3. 3. 6. 3. 4. 3. 6. 3.\n",
      " 6. 3. 6. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 1. 4. 3. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 1. 3.\n",
      " 3. 1. 3. 3. 3. 3. 3. 3. 1. 3. 3. 3. 2. 2. 6. 3. 3. 3. 3. 1. 3. 3. 2. 4.\n",
      " 3. 3. 3. 1. 1. 3. 3. 2. 1. 3. 6. 3. 6. 3. 1. 3. 2. 6. 3. 3. 3. 1. 3. 3.\n",
      " 3. 1. 1. 3. 2. 3. 1. 6. 1. 1. 3. 3. 3. 4. 2. 3. 6. 1. 1. 3. 2. 1. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 1. 3. 6. 3. 2. 3. 3. 2. 3. 3. 3. 3.\n",
      " 6. 3. 3. 1. 3. 3. 3. 3. 3. 3. 3. 2. 3. 3. 2. 3. 3. 1. 3. 1. 3. 1. 6. 3.\n",
      " 6. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 1. 3. 3. 3. 3. 3. 3. 6.\n",
      " 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 1.\n",
      " 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 6. 1. 2. 3. 3. 3. 3. 1. 3. 1. 6. 1. 3.\n",
      " 1. 3. 3. 3. 6. 1. 6. 3. 3. 3. 3. 3. 6. 1. 3. 2. 3. 6. 3. 3. 3. 3. 3. 5.\n",
      " 3. 1. 3. 3. 3. 3. 3. 3. 3. 6. 3. 3. 3. 1. 3. 6. 4. 1. 3. 3. 3. 3. 3. 3.\n",
      " 6. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 6. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 2. 3. 3. 2. 3. 3. 3. 3. 1. 3. 3. 3. 1. 3. 3. 3. 3. 1. 3. 3. 4. 3.\n",
      " 3. 3. 2. 1. 1. 3. 3. 3. 3. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 1. 1.\n",
      " 3. 3. 6. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 6. 2. 3. 2. 1. 3.\n",
      " 3. 3. 2. 3. 3. 6. 4. 3. 1. 3. 3. 3. 3. 3. 3. 3. 1. 6. 1. 1. 3. 1. 3. 3.\n",
      " 1. 6. 1. 3. 3. 3. 3. 6. 3. 3. 3. 3. 3. 3. 3. 3. 3. 2. 3. 3. 3. 6. 1. 5.\n",
      " 3. 3. 1. 5. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 5. 3. 6. 3. 3. 3. 3. 3. 1.\n",
      " 3. 3. 2. 3. 3. 3. 3. 3. 6. 1. 3. 3. 1. 1. 5. 6. 2. 3. 1. 3. 3. 3. 3. 3.\n",
      " 3. 3. 2. 3. 3. 3. 2. 3. 1. 6. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 6.\n",
      " 3. 3. 3. 3. 6. 3. 3. 3. 3. 1. 3. 3. 3. 3. 1. 2. 3. 3. 3. 3. 3. 3. 1. 1.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 2. 3. 3. 3. 3. 1. 3. 3. 3. 3. 3. 4. 3. 3. 1.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 1. 3. 3. 1. 1. 3. 3.]\n",
      "Accuracy score:               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.73      0.69      0.71      2422\n",
      "         2.0       0.85      0.83      0.84      3434\n",
      "         3.0       0.72      0.76      0.74      3323\n",
      "         4.0       0.92      0.91      0.92      2409\n",
      "         5.0       0.90      0.89      0.90      1838\n",
      "         6.0       0.84      0.84      0.84      4927\n",
      "\n",
      "    accuracy                           0.82     18353\n",
      "   macro avg       0.83      0.82      0.82     18353\n",
      "weighted avg       0.82      0.82      0.82     18353\n",
      "\n",
      "Accuracy score:               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.71      0.60      0.65       605\n",
      "         2.0       0.75      0.75      0.75       858\n",
      "         3.0       0.63      0.82      0.72       831\n",
      "         4.0       0.92      0.92      0.92       603\n",
      "         5.0       0.89      0.83      0.86       460\n",
      "         6.0       0.81      0.73      0.77      1226\n",
      "\n",
      "    accuracy                           0.77      4583\n",
      "   macro avg       0.79      0.78      0.78      4583\n",
      "weighted avg       0.78      0.77      0.77      4583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Semi-supervised learning\n",
    "\n",
    "xtrain = f[:,:-1] #14k, 54 feature vector 15x54\n",
    "ytrain = f[:,-1] #14k , 1 label per image 6 classes \n",
    "\n",
    "xval = v[:,:-1]\n",
    "yval = v[:,-1]\n",
    "\n",
    "xtest = t[:,:-1]\n",
    "ytest = t[:,-1]\n",
    "\n",
    "xtotal = np.vstack((xtrain, xval))\n",
    "ytotal = np.concatenate((ytrain, yval))\n",
    "ytotal_unlabled = ytotal.copy()\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# Creating unlabeled data by assigning \"-1\"\n",
    "unl_pts = rng.rand(ytotal.shape[0]) > 0.1 #[0.012,,0.8...] size(ytotal) => [true,false,...] size(ytotal)\n",
    "ytotal_unlabled[unl_pts] = -1 #assign true values to -1\n",
    "count = 0\n",
    "for e in ytotal_unlabled:\n",
    "    if(e==-1):\n",
    "        count += 1\n",
    "print(\"Inlabled points,\", unl_pts)\n",
    "print(\"Size of labels:\" , ytotal_unlabled)\n",
    "print(\"Count of unlabled data:\" , count)\n",
    "# Semi-Supervised learning\n",
    "dtc = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "\n",
    "# Set the threshold for predicted probabilities\n",
    "threshold = 0.99  # Example threshold value\n",
    "max_iter = 1000  # Example maximum number of iterations\n",
    "\n",
    "lbl = semi_supervised.SelfTrainingClassifier(dtc, threshold=threshold,max_iter=None)\n",
    "lbl.fit(xtotal, ytotal_unlabled)\n",
    "\n",
    "y_pred = lbl.predict(xtotal)\n",
    "y_pred2 = lbl.predict(xtest)\n",
    "\n",
    "\n",
    "# with np.printoptions(threshold=1000):\n",
    "#     print(\"Labels for training\", xtotal)\n",
    "#     print(\"Actual labled data: \", ytotal[5000:6000])\n",
    "#     print(\"Unlabled data: \", ytotal_unlabled[5000:6000])\n",
    "#     print(\"Semi Supervised Labels: \", y_pred[5000:6000])\n",
    "\n",
    "#Getting accuracy for original lables vs preicted labels\n",
    "\n",
    "acu = classification_report(ytotal,y_pred) #Accuracy labled vs unlabled data\n",
    "acu2 = classification_report(ytest,y_pred2) #Accuracy on testing data\n",
    "\n",
    "print(\"Accuracy score:\", acu)\n",
    "print(\"Accuracy score:\", acu2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5fc29a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.68      0.69      0.69       485\n",
      "         2.0       0.88      0.91      0.89       686\n",
      "         3.0       0.75      0.77      0.76       665\n",
      "         4.0       0.91      0.93      0.92       482\n",
      "         5.0       0.96      0.96      0.96       368\n",
      "         6.0       0.91      0.85      0.88       991\n",
      "\n",
      "    accuracy                           0.85      3677\n",
      "   macro avg       0.85      0.85      0.85      3677\n",
      "weighted avg       0.85      0.85      0.85      3677\n",
      "\n",
      "Confusion Matrix:\n",
      " [[336  10 121  12   2   4]\n",
      " [ 10 623  25   1   7  20]\n",
      " [ 76  25 515  10   1  38]\n",
      " [  9   2   7 448   0  16]\n",
      " [  0  11   0   0 354   3]\n",
      " [ 60  41  23  20   5 842]]\n"
     ]
    }
   ],
   "source": [
    "#training model\n",
    "\n",
    "xtrain = f[:,:-1] #14k, 54 feature vector 15x54\n",
    "ytrain = f[:,-1] #14k , 1 label per image 6 classes \n",
    "\n",
    "xval = v[:,:-1]\n",
    "yval = v[:,-1]\n",
    "\n",
    "dtc = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "dtc.fit(xtrain, ytrain)\n",
    "\n",
    "y_pred = dtc.predict(xval)\n",
    "\n",
    "print(classification_report(yval, y_pred)) #metrics values\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(yval, y_pred)) #confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5fd28c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.74      0.66      0.70       605\n",
      "         2.0       0.80      0.89      0.84       858\n",
      "         3.0       0.73      0.83      0.78       831\n",
      "         4.0       0.94      0.95      0.95       603\n",
      "         5.0       0.93      0.97      0.95       460\n",
      "         6.0       0.89      0.77      0.83      1226\n",
      "\n",
      "    accuracy                           0.83      4583\n",
      "   macro avg       0.84      0.84      0.84      4583\n",
      "weighted avg       0.84      0.83      0.83      4583\n",
      "\n",
      "Confusion Matrix:\n",
      " [[402  12 171   6   3  11]\n",
      " [ 20 763  15   1   5  54]\n",
      " [ 72  24 687  15   2  31]\n",
      " [  4   1  12 573   1  12]\n",
      " [  0  11   0   0 446   3]\n",
      " [ 44 148  53  14  25 942]]\n",
      "Number of leaves: 729\n",
      "Number of nodes: 1457\n",
      "Max of number of nodes: 18\n"
     ]
    }
   ],
   "source": [
    "# #Model Training and saving best model\n",
    "\n",
    "xtest = t[:,:-1]\n",
    "ytest = t[:,-1]\n",
    "\n",
    "y_pred = dtc.predict(xtest)\n",
    "\n",
    "print(classification_report(ytest, y_pred)) #metrics values\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(ytest, y_pred)) #confusion matrix\n",
    "\n",
    "num_leaves = dtc.tree_.n_leaves\n",
    "num_nodes = dtc.tree_.node_count\n",
    "maxd = dtc.tree_.max_depth\n",
    "\n",
    "print(\"Number of leaves:\", num_leaves)\n",
    "print(\"Number of nodes:\", num_nodes)\n",
    "print(\"Max of depth:\", maxd)\n",
    "\n",
    "#plotting tree\n",
    "# tree.plot_tree(dtc)\n",
    "# elements = [\"color\"] * 24 + [\"shape\"] * 20 + [\"texture\"] * 10\n",
    "# dot_data = tree.export_graphviz(dtc, out_file=None, feature_names=elements, class_names=['Banana', 'Apple',\"Orange\",\"Plum\",\"Mango\",\"Kiwi\"],filled=True, rounded=True)\n",
    "# graph = graphviz.Source(dot_data)\n",
    "# graph.render(\"mytree\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f686fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c4d38d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
