{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9595516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "import graphviz\n",
    "#I Have problems with packages try running without this command \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import local_binary_pattern\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pathlib\n",
    "from torchvision.transforms import ToTensor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3365b4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "#checking for device\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235358a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_features(image, target_size):\n",
    "    # Convert the image to the HSV color space\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the number of bins for each channel in the histogram\n",
    "    hue_bins = 8\n",
    "    saturation_bins = 8\n",
    "    value_bins = 8\n",
    "\n",
    "    # Calculate the color histogram for each channel\n",
    "    hue_hist = cv2.calcHist([hsv_image], [0], None, [hue_bins], [0, 180])\n",
    "    saturation_hist = cv2.calcHist([hsv_image], [1], None, [saturation_bins], [0, 256])\n",
    "    value_hist = cv2.calcHist([hsv_image], [2], None, [value_bins], [0, 256])\n",
    "\n",
    "    # Normalize the histograms\n",
    "    cv2.normalize(hue_hist, hue_hist, 0, 1, cv2.NORM_MINMAX)\n",
    "    cv2.normalize(saturation_hist, saturation_hist, 0, 1, cv2.NORM_MINMAX)\n",
    "    cv2.normalize(value_hist, value_hist, 0, 1, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    color_features = np.concatenate((hue_hist.flatten(), saturation_hist.flatten(), value_hist.flatten()))\n",
    "\n",
    "    # Resize the color features to the target size\n",
    "    if len(color_features) < target_size:\n",
    "        color_features = np.pad(color_features, (0, target_size - len(color_features)), mode='constant')\n",
    "    elif len(color_features) > target_size:\n",
    "        color_features = color_features[:target_size]\n",
    "\n",
    "    #print(\"Color features:\", color_features.shape)\n",
    "\n",
    "    return color_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c57fa583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_shape_features(image, target_size):\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply binary thresholding to obtain a binary image\n",
    "    _, binary_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours in the binary image\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Initialize a list to store shape features\n",
    "    shape_features = []\n",
    "\n",
    "    # Iterate over the contours\n",
    "    for contour in contours:\n",
    "        # Calculate contour-based features\n",
    "        area = cv2.contourArea(contour)\n",
    "        perimeter = cv2.arcLength(contour, True)\n",
    "        _, _, width, height = cv2.boundingRect(contour)\n",
    "        aspect_ratio = width / float(height) if height != 0 else 0\n",
    "        circularity = 4 * np.pi * area / (perimeter ** 2) if perimeter != 0 else 0\n",
    "\n",
    "        # Append the shape features to the list\n",
    "        shape_features.extend([area, perimeter, aspect_ratio, circularity])\n",
    "\n",
    "    # Convert the shape features list to a numpy array\n",
    "    shape_features = np.array(shape_features)\n",
    "\n",
    "    # Resize the shape features to the target size\n",
    "    if len(shape_features) < target_size:\n",
    "        shape_features = np.pad(shape_features, (0, target_size - len(shape_features)), mode='constant')\n",
    "    elif len(shape_features) > target_size:\n",
    "        shape_features = shape_features[:target_size]\n",
    "\n",
    "    #print(\"Shape features:\", shape_features.shape)\n",
    "    return shape_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71762d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texture_features(image, target_size):\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate the Local Binary Pattern (LBP) for the grayscale image\n",
    "    radius = 1\n",
    "    n_points = 8 * radius\n",
    "    lbp_image = local_binary_pattern(gray_image, n_points, radius, method='uniform')\n",
    "\n",
    "    # Calculate the histogram of the LBP image\n",
    "    hist, _ = np.histogram(lbp_image.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "\n",
    "    # Normalize the histogram\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-7)\n",
    "\n",
    "    # Flatten and return the histogram as the texture feature vector\n",
    "    texture_features = hist.flatten()\n",
    "\n",
    "    # Resize the texture features to the target size\n",
    "    if len(texture_features) < target_size:\n",
    "        texture_features = np.pad(texture_features, (0, target_size - len(texture_features)), mode='constant')\n",
    "    elif len(texture_features) > target_size:\n",
    "        texture_features = texture_features[:target_size]\n",
    "\n",
    "    #print(\"Texture features:\", texture_features.shape)\n",
    "\n",
    "    return texture_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f290db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(image):\n",
    "    # Load and preprocess the image\n",
    "    # Assuming image is already loaded or you can use OpenCV to load it\n",
    "    preprocessed_image = image\n",
    "    #print(preprocessed_image)\n",
    "\n",
    "    # Extract features using different methods\n",
    "    color_features = extract_color_features(preprocessed_image,24)\n",
    "    shape_features = extract_shape_features(preprocessed_image,20)\n",
    "    texture_features = extract_texture_features(preprocessed_image,10)\n",
    "\n",
    "    # Combine the features into a single vector\n",
    "    #print(color_features.shape, shape_features.shape, texture_features.shape)\n",
    "    combined_features = np.concatenate((color_features, shape_features, texture_features),axis=None)\n",
    "\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9d8af6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#folder_path = \"/Users/hadi/Desktop/Concordia/Comp 6721/AIproject/fruits/training/Kiwi_Training\"\n",
    "\n",
    "def generate_features(image):\n",
    "    #image = cv2.imread(image_path)\n",
    "    #new_size = (32, 32)\n",
    "    #image = cv2.resize(image, new_size)\n",
    "    combined_features = combine_features(image)\n",
    "    return combined_features\n",
    "\n",
    "def check_label(element):\n",
    "    if element == 'Banana_Training' or element == \"Banana_Test\" or element == \"Banana_Validation\":\n",
    "        return 1\n",
    "    elif element == 'Kiwi_Training' or element == \"Kiwi_Test\" or element == \"Kiwi_Validation\":\n",
    "        return 2\n",
    "    elif element == 'Mango_Training' or element == \"Mango_Test\" or element == \"Mango_Validation\":\n",
    "        return 3\n",
    "    elif element == 'Orange_Training' or element == \"Orange_Test\" or element == \"Orange_Validation\":\n",
    "        return 4\n",
    "    elif element == 'Plum_Training' or element == \"Plum_Test\" or element == \"Plum_Validation\":\n",
    "        return 5\n",
    "    elif element == 'Apple_Training' or element == \"Apple_Test\" or  element == \"Apple_Validation\":\n",
    "        return 6\n",
    "    else:\n",
    "        return 0  # Return 0 if the element is not found in the list\n",
    "\n",
    "def loadImages(folder_path,class_):\n",
    "    #print(folder_path)\n",
    "    folder_path = folder_path\n",
    "    file_list = os.listdir(folder_path)\n",
    "    class_features = np.empty((0,55))\n",
    "    for file_name in file_list:\n",
    "         if file_name.endswith(\".jpg\") or file_name.endswith(\".png\"):\n",
    "             image_path = os.path.join(folder_path, file_name)\n",
    "             # Perform your image processing tasks here\n",
    "             image = cv2.imread(image_path)\n",
    "             new_size = (32, 32)\n",
    "             image = cv2.resize(image, new_size)\n",
    "             combined_features = combine_features(image)\n",
    "             #print(check_label(class_))\n",
    "             combined_features = np.append(combined_features, check_label(class_))\n",
    "             combined_features = np.expand_dims(combined_features, axis=0)\n",
    "             class_features = np.append(class_features, combined_features,axis=0)\n",
    "    #print(class_features[0])\n",
    "    return class_features\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "#Print the shape of the combined feature vector\n",
    "#loadImages(folder_path,\"Kiwi_Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cb10169",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/Users/hadi/Desktop/Concordia/Comp 6721/AIproject/fruits/training\"\n",
    "test_path = \"/Users/hadi/Desktop/Concordia/Comp 6721/AIproject/fruits/testing\"\n",
    "val_path = \"/Users/hadi/Desktop/Concordia/Comp 6721/AIproject/fruits/validation\"\n",
    "\n",
    "root_training=pathlib.Path(train_path)\n",
    "root_testing=pathlib.Path(test_path)\n",
    "root_val = pathlib.Path(val_path)\n",
    "\n",
    "\n",
    "def generate_feature_vector(root,path):\n",
    "    classes = []\n",
    "    features = np.empty((0,55))\n",
    "    labels = []\n",
    "    for class_dir in root.iterdir():\n",
    "        class_ = class_dir.name.split('/')[-1]\n",
    "        print(class_)\n",
    "        path_ = path +\"/\"\n",
    "        if(class_!=\".DS_Store\"):\n",
    "            print(class_)\n",
    "            path_ = path+\"/\"+class_\n",
    "            temp = loadImages(path_,class_)\n",
    "            path_ = \"\"\n",
    "            classes.append(class_)\n",
    "            features = np.append(features, temp,axis=0)\n",
    "                \n",
    "    return features \n",
    "#classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "#print(classes) #['Banana_Training', 'Kiwi_Training', 'Mango_Training', 'Orange_Training', 'Plum_Training', 'Apple_Training']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6580e34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banana_Training\n",
      "Banana_Training\n",
      ".DS_Store\n",
      "Kiwi_Training\n",
      "Kiwi_Training\n",
      "Mango_Training\n",
      "Mango_Training\n",
      "Orange_Training\n",
      "Orange_Training\n",
      "Plum_Training\n",
      "Plum_Training\n",
      "Apple_Training\n",
      "Apple_Training\n",
      ".DS_Store\n",
      "Kiwi_Test\n",
      "Kiwi_Test\n",
      "Plum_Test\n",
      "Plum_Test\n",
      "Orange_Test\n",
      "Orange_Test\n",
      "Apple_Test\n",
      "Apple_Test\n",
      "Mango_Test\n",
      "Mango_Test\n",
      "Banana_Test\n",
      "Banana_Test\n",
      ".DS_Store\n",
      "Mango_Validation\n",
      "Mango_Validation\n",
      "Plum_Validation\n",
      "Plum_Validation\n",
      "Banana_Validation\n",
      "Banana_Validation\n",
      "Apple_Validation\n",
      "Apple_Validation\n",
      "Kiwi_Validation\n",
      "Kiwi_Validation\n",
      "Orange_Validation\n",
      "Orange_Validation\n",
      "(14676, 55)\n",
      "(4583, 55)\n",
      "(3677, 55)\n"
     ]
    }
   ],
   "source": [
    "#calculate size of training and testing images \n",
    "\n",
    "f = generate_feature_vector(root_training,train_path) #total training features \n",
    "t = generate_feature_vector(root_testing,test_path) #total testing features\n",
    "v = generate_feature_vector(root_val,val_path) #total val features\n",
    "\n",
    "print(f.shape)\n",
    "print(t.shape)\n",
    "print(v.shape)\n",
    "#train_count = len(glob.glob(train_path+\"/**/*.png\"))\n",
    "#test_count = len(glob.glob(test_path+\"/**/*.png\"))\n",
    "\n",
    "#print(train_count,test_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f186bcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels for training "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training model\n",
    "\n",
    "xtrain = f[:,:-1]\n",
    "ytrain = f[:,-1]\n",
    "xval = v[:,:-1]\n",
    "yval = v[:,-1]\n",
    "\n",
    "xtotal = np.vstack((xtrain, xval))\n",
    "ytotal = np.concatenate((ytrain, yval))\n",
    "\n",
    "# print(\"Features for training:\" ,xtrain)\n",
    "# print(\"Labels for training\", ytrain)\n",
    "# print(\"Features for testing:\", xtest)\n",
    "# with np.printoptions(threshold=sys.maxsize):\n",
    "#     print(\"Labels for training\", xtotal)\n",
    "#     print(\"Labels for testing\", ytotal)\n",
    "\n",
    "dtc = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "dtc.fit(xtotal, ytotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b400f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypertune parameters\n",
    "\n",
    "# xval = f[:,:-1]\n",
    "# yval = f[:,-1]\n",
    "\n",
    "#google how to hypertune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d202c67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.75      0.70      0.73       605\n",
      "         2.0       0.86      0.90      0.88       858\n",
      "         3.0       0.77      0.87      0.82       831\n",
      "         4.0       0.96      0.96      0.96       603\n",
      "         5.0       0.93      0.98      0.95       460\n",
      "         6.0       0.92      0.83      0.87      1226\n",
      "\n",
      "    accuracy                           0.86      4583\n",
      "   macro avg       0.87      0.87      0.87      4583\n",
      "weighted avg       0.87      0.86      0.86      4583\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 425    9  159    1    1   10]\n",
      " [  14  768   21    1    8   46]\n",
      " [  71   10  726    9    0   15]\n",
      " [   2    0    5  577    1   18]\n",
      " [   0    6    0    0  449    5]\n",
      " [  54   97   28   10   23 1014]]\n"
     ]
    }
   ],
   "source": [
    "# #Model Training and saving best model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xtest = t[:,:-1]\n",
    "ytest = t[:,-1]\n",
    "\n",
    "y_pred = dtc.predict(xtest)\n",
    "\n",
    "print(classification_report(ytest, y_pred)) #metrics values\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(ytest, y_pred)) #confusion matrix\n",
    "\n",
    "#plotting tree\n",
    "# tree.plot_tree(dtc)\n",
    "# elements = [\"color\"] * 24 + [\"shape\"] * 20 + [\"texture\"] * 10\n",
    "# dot_data = tree.export_graphviz(dtc, out_file=None, feature_names=elements, class_names=['Banana', 'Apple',\"Orange\",\"Plum\",\"Mango\",\"Kiwi\"],filled=True, rounded=True)\n",
    "# graph = graphviz.Source(dot_data)\n",
    "# graph.render(\"mytree\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb60b592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
