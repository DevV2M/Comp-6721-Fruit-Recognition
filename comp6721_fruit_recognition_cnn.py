# -*- coding: utf-8 -*-
"""COMP6721_Fruit Recognition_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12uwE2kweqd1E5SFpOidCK2K04aMUPGZj
"""

#Load libraries
import os
import numpy as np
import torch
import glob
import torch.nn as nn
from torchvision.transforms import transforms
from torch.utils.data import DataLoader
from torch.optim import Adam
from torch.autograd import Variable
import torchvision
import pathlib
from google.colab import drive #used to access files
drive.mount('/content/gdrive')

#checking for device
device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(device)

#Transforms
transformer=transforms.Compose([
    transforms.Resize((150,150)), #recommended size with 3x3 filters bigger images need larger filters
    transforms.RandomHorizontalFlip(), #chance of image being flipped is 0.5 to add variety to data set
    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors (Pytorch uses tensor instead of numoy to manipulate images)
    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std
                        [0.5,0.5,0.5])
])

#Dataloader

#Path for training and testing directory
train_path='gdrive/My Drive/fruits/training/'
test_path='gdrive/My Drive/fruits/testing/'

train_loader=DataLoader(
    torchvision.datasets.ImageFolder(train_path,transform=transformer),
    batch_size=64, shuffle=True
)
#by deault it was 64 but ta said use 32 (Personally I think 64 is better because we have lots of images)

test_loader=DataLoader(
    torchvision.datasets.ImageFolder(test_path,transform=transformer),
    batch_size=32, shuffle=True
)

#Returning classes
root=pathlib.Path(train_path)
classes = []
for j in root.iterdir():
    if j.name.split('/')[-1] != ".DS_Store":
        classes.append(j.name.split('/')[-1])

print(classes)

#Building CNN netwrok



class ConvNet(nn.Module):
    def __init__(self,num_classes=6):
        super(ConvNet,self).__init__()

        #Output size after convolution filter
        #((w-f+2P)/s) +1 w: width (32) - f: kernel size (filter) - p:padding - s:stride

        #Input shape= (32,3,32,32) 32: batch size - 3: rgb channels - 32x32: image size

        self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)
        #Shape= (256,12,150,150)
        self.bn1=nn.BatchNorm2d(num_features=12)
        #Shape= (256,12,150,150)
        self.relu1=nn.ReLU()
        #Shape= (256,12,150,150)

        self.pool=nn.MaxPool2d(kernel_size=2)
        #Reduce the image size be factor 2
        #Shape= (256,12,75,75)


        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)
        #Shape= (256,20,75,75)
        self.relu2=nn.ReLU()
        #Shape= (256,20,75,75)



        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)
        #Shape= (256,32,75,75)
        self.bn3=nn.BatchNorm2d(num_features=32)
        #Shape= (256,32,75,75)
        self.relu3=nn.ReLU()
        #Shape= (256,32,75,75)


        self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes)



        #Feed forwad function

    def forward(self,input):
        output=self.conv1(input)
        output=self.bn1(output)
        output=self.relu1(output)

        output=self.pool(output)

        output=self.conv2(output)
        output=self.relu2(output)

        output=self.conv3(output)
        output=self.bn3(output)
        output=self.relu3(output)


            #Above output will be in matrix form, with shape (256,32,75,75)

        output=output.view(-1,32*75*75)


        output=self.fc(output)

        return output

model=ConvNet(num_classes=6).to(device)

#Optmizer and loss function
optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)
loss_function=nn.CrossEntropyLoss()

num_epochs=10

#calculating the size of training and testing images
train_count=len(glob.glob(train_path+'/**/*.png'))
test_count=len(glob.glob(test_path+'/**/*.png'))
print(train_count,test_count)

#Model training and saving best model

best_accuracy=0.0
metrics = np.empty((0, 3))  # Initialize an empty matrix

for epoch in range(num_epochs):

    #Evaluation and training on training dataset
    model.train()
    train_accuracy=0.0
    train_loss=0.0

    for i, (images,labels) in enumerate(train_loader):
        if torch.cuda.is_available():
            images=Variable(images.cuda())
            labels=Variable(labels.cuda())

        optimizer.zero_grad()

        outputs=model(images)
        loss=loss_function(outputs,labels)
        loss.backward()
        optimizer.step()


        train_loss+= loss.cpu().data*images.size(0)
        _,prediction=torch.max(outputs.data,1)

        train_accuracy+=int(torch.sum(prediction==labels.data))

    train_accuracy=train_accuracy/train_count
    train_loss=train_loss/train_count


    # Evaluation on testing dataset
    model.eval()

    test_accuracy=0.0
    for i, (images,labels) in enumerate(test_loader):
        if torch.cuda.is_available():
            images=Variable(images.cuda())
            labels=Variable(labels.cuda())

        outputs=model(images)
        _,prediction=torch.max(outputs.data,1)
        test_accuracy+=int(torch.sum(prediction==labels.data))

    test_accuracy=test_accuracy/test_count

    metric_per_epoch = np.array([train_loss,train_accuracy,test_accuracy])  #Array that holds value of metrics per epoch
    metrics = np.vstack((metrics, metric_per_epoch))

    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))

    #Save the best model
    if test_accuracy>best_accuracy:
        torch.save(model.state_dict(),'best_checkpoint.model')
        best_accuracy=test_accuracy

import matplotlib.pyplot as plt

train_count_orange=len(glob.glob(train_path+'/Orange_Training/*.png'))
train_count_kiwi=len(glob.glob(train_path+'/Kiwi_Training/*.png'))
train_count_apple=len(glob.glob(train_path+'/Apple_Training/*.png'))
train_count_plum=len(glob.glob(train_path+'/Plum_Training/*.png'))
train_count_mango=len(glob.glob(train_path+'/Mango_Training/*.png'))
train_count_banana=len(glob.glob(train_path+'/Banana_Training/*.png'))

# Count the frequency of each label
label_counts = {"Orange":train_count_orange,"Kiwi":train_count_kiwi,"Apple":train_count_apple,"Plum":train_count_plum,"Mango":train_count_mango,"Banana":train_count_banana}

# Extract the labels and frequencies
x = list(label_counts.keys())
y = list(label_counts.values())

# Plotting the graph
plt.bar(x, y)
plt.xlabel('Classes')
plt.ylabel('Frequency')
plt.title('Class Frequency')
plt.show()

#Plot for the loss curve

loss_values = []

for m in metrics:
  loss_values.append(m[0])

# Generate x-axis values for epochs
epochs = range(1, len(loss_values) + 1)

# Plotting the loss curve
plt.plot(epochs, loss_values, 'b-o')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Curve')
plt.grid(True)
plt.show()

#plot for the test vs training accuracy to check overfiiting

training_accuracy = []
test_accuracy = []

for m in metrics:
  training_accuracy.append(m[1])
  test_accuracy.append(m[2])

# Generate x-axis values for epochs
epochs = range(1, len(training_accuracy) + 1)

# Plotting the test vs training accuracy
plt.plot(epochs, training_accuracy, 'b-o', label='Training Accuracy')
plt.plot(epochs, test_accuracy, 'r-o', label='Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Test vs Training Accuracy')
plt.legend()
plt.grid(True)
plt.show()